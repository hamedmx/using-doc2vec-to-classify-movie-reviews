{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Doc2Vec to classify movie reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "import tensorflow as tf\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download the IMDB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "imdb = tf.keras.datasets.imdb\n",
    "(train_reviews, train_labels), (test_reviews, test_labels) = imdb.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 14,\n",
       " 22,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 973,\n",
       " 1622,\n",
       " 1385,\n",
       " 65,\n",
       " 458,\n",
       " 4468,\n",
       " 66,\n",
       " 3941,\n",
       " 4,\n",
       " 173,\n",
       " 36,\n",
       " 256,\n",
       " 5,\n",
       " 25,\n",
       " 100,\n",
       " 43,\n",
       " 838,\n",
       " 112,\n",
       " 50,\n",
       " 670,\n",
       " 22665,\n",
       " 9,\n",
       " 35,\n",
       " 480,\n",
       " 284,\n",
       " 5,\n",
       " 150,\n",
       " 4,\n",
       " 172,\n",
       " 112,\n",
       " 167,\n",
       " 21631,\n",
       " 336,\n",
       " 385,\n",
       " 39,\n",
       " 4,\n",
       " 172,\n",
       " 4536,\n",
       " 1111,\n",
       " 17,\n",
       " 546,\n",
       " 38,\n",
       " 13,\n",
       " 447,\n",
       " 4,\n",
       " 192,\n",
       " 50,\n",
       " 16,\n",
       " 6,\n",
       " 147,\n",
       " 2025,\n",
       " 19,\n",
       " 14,\n",
       " 22,\n",
       " 4,\n",
       " 1920,\n",
       " 4613,\n",
       " 469,\n",
       " 4,\n",
       " 22,\n",
       " 71,\n",
       " 87,\n",
       " 12,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 38,\n",
       " 76,\n",
       " 15,\n",
       " 13,\n",
       " 1247,\n",
       " 4,\n",
       " 22,\n",
       " 17,\n",
       " 515,\n",
       " 17,\n",
       " 12,\n",
       " 16,\n",
       " 626,\n",
       " 18,\n",
       " 19193,\n",
       " 5,\n",
       " 62,\n",
       " 386,\n",
       " 12,\n",
       " 8,\n",
       " 316,\n",
       " 8,\n",
       " 106,\n",
       " 5,\n",
       " 4,\n",
       " 2223,\n",
       " 5244,\n",
       " 16,\n",
       " 480,\n",
       " 66,\n",
       " 3785,\n",
       " 33,\n",
       " 4,\n",
       " 130,\n",
       " 12,\n",
       " 16,\n",
       " 38,\n",
       " 619,\n",
       " 5,\n",
       " 25,\n",
       " 124,\n",
       " 51,\n",
       " 36,\n",
       " 135,\n",
       " 48,\n",
       " 25,\n",
       " 1415,\n",
       " 33,\n",
       " 6,\n",
       " 22,\n",
       " 12,\n",
       " 215,\n",
       " 28,\n",
       " 77,\n",
       " 52,\n",
       " 5,\n",
       " 14,\n",
       " 407,\n",
       " 16,\n",
       " 82,\n",
       " 10311,\n",
       " 8,\n",
       " 4,\n",
       " 107,\n",
       " 117,\n",
       " 5952,\n",
       " 15,\n",
       " 256,\n",
       " 4,\n",
       " 31050,\n",
       " 7,\n",
       " 3766,\n",
       " 5,\n",
       " 723,\n",
       " 36,\n",
       " 71,\n",
       " 43,\n",
       " 530,\n",
       " 476,\n",
       " 26,\n",
       " 400,\n",
       " 317,\n",
       " 46,\n",
       " 7,\n",
       " 4,\n",
       " 12118,\n",
       " 1029,\n",
       " 13,\n",
       " 104,\n",
       " 88,\n",
       " 4,\n",
       " 381,\n",
       " 15,\n",
       " 297,\n",
       " 98,\n",
       " 32,\n",
       " 2071,\n",
       " 56,\n",
       " 26,\n",
       " 141,\n",
       " 6,\n",
       " 194,\n",
       " 7486,\n",
       " 18,\n",
       " 4,\n",
       " 226,\n",
       " 22,\n",
       " 21,\n",
       " 134,\n",
       " 476,\n",
       " 26,\n",
       " 480,\n",
       " 5,\n",
       " 144,\n",
       " 30,\n",
       " 5535,\n",
       " 18,\n",
       " 51,\n",
       " 36,\n",
       " 28,\n",
       " 224,\n",
       " 92,\n",
       " 25,\n",
       " 104,\n",
       " 4,\n",
       " 226,\n",
       " 65,\n",
       " 16,\n",
       " 38,\n",
       " 1334,\n",
       " 88,\n",
       " 12,\n",
       " 16,\n",
       " 283,\n",
       " 5,\n",
       " 16,\n",
       " 4472,\n",
       " 113,\n",
       " 103,\n",
       " 32,\n",
       " 15,\n",
       " 16,\n",
       " 5345,\n",
       " 19,\n",
       " 178,\n",
       " 32]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_reviews[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create a function to decode reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vocab = imdb.get_word_index()\n",
    "vocab = {k:(v + 3) for k, v in vocab.items()}\n",
    "vocab[\"<PAD>\"] = 0\n",
    "vocab[\"<START>\"] = 1\n",
    "vocab[\"<UNK>\"] = 2\n",
    "vocab[\"<UNUSED>\"] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "530"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[\"brilliant\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vocab_inv =  dict([(value, key) for (key, value) in vocab.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'incredible'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_inv[1048]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def decode_review(review):\n",
    "    return [vocab_inv.get(i, \"?\") for i in review]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<START>',\n",
       " 'this',\n",
       " 'film',\n",
       " 'was',\n",
       " 'just',\n",
       " 'brilliant',\n",
       " 'casting',\n",
       " 'location',\n",
       " 'scenery',\n",
       " 'story',\n",
       " 'direction',\n",
       " \"everyone's\",\n",
       " 'really',\n",
       " 'suited',\n",
       " 'the',\n",
       " 'part',\n",
       " 'they',\n",
       " 'played',\n",
       " 'and',\n",
       " 'you',\n",
       " 'could',\n",
       " 'just',\n",
       " 'imagine',\n",
       " 'being',\n",
       " 'there',\n",
       " 'robert',\n",
       " \"redford's\",\n",
       " 'is',\n",
       " 'an',\n",
       " 'amazing',\n",
       " 'actor',\n",
       " 'and',\n",
       " 'now',\n",
       " 'the',\n",
       " 'same',\n",
       " 'being',\n",
       " 'director',\n",
       " \"norman's\",\n",
       " 'father',\n",
       " 'came',\n",
       " 'from',\n",
       " 'the',\n",
       " 'same',\n",
       " 'scottish',\n",
       " 'island',\n",
       " 'as',\n",
       " 'myself',\n",
       " 'so',\n",
       " 'i',\n",
       " 'loved',\n",
       " 'the',\n",
       " 'fact',\n",
       " 'there',\n",
       " 'was',\n",
       " 'a',\n",
       " 'real',\n",
       " 'connection',\n",
       " 'with',\n",
       " 'this',\n",
       " 'film',\n",
       " 'the',\n",
       " 'witty',\n",
       " 'remarks',\n",
       " 'throughout',\n",
       " 'the',\n",
       " 'film',\n",
       " 'were',\n",
       " 'great',\n",
       " 'it',\n",
       " 'was',\n",
       " 'just',\n",
       " 'brilliant',\n",
       " 'so',\n",
       " 'much',\n",
       " 'that',\n",
       " 'i',\n",
       " 'bought',\n",
       " 'the',\n",
       " 'film',\n",
       " 'as',\n",
       " 'soon',\n",
       " 'as',\n",
       " 'it',\n",
       " 'was',\n",
       " 'released',\n",
       " 'for',\n",
       " 'retail',\n",
       " 'and',\n",
       " 'would',\n",
       " 'recommend',\n",
       " 'it',\n",
       " 'to',\n",
       " 'everyone',\n",
       " 'to',\n",
       " 'watch',\n",
       " 'and',\n",
       " 'the',\n",
       " 'fly',\n",
       " 'fishing',\n",
       " 'was',\n",
       " 'amazing',\n",
       " 'really',\n",
       " 'cried',\n",
       " 'at',\n",
       " 'the',\n",
       " 'end',\n",
       " 'it',\n",
       " 'was',\n",
       " 'so',\n",
       " 'sad',\n",
       " 'and',\n",
       " 'you',\n",
       " 'know',\n",
       " 'what',\n",
       " 'they',\n",
       " 'say',\n",
       " 'if',\n",
       " 'you',\n",
       " 'cry',\n",
       " 'at',\n",
       " 'a',\n",
       " 'film',\n",
       " 'it',\n",
       " 'must',\n",
       " 'have',\n",
       " 'been',\n",
       " 'good',\n",
       " 'and',\n",
       " 'this',\n",
       " 'definitely',\n",
       " 'was',\n",
       " 'also',\n",
       " 'congratulations',\n",
       " 'to',\n",
       " 'the',\n",
       " 'two',\n",
       " 'little',\n",
       " \"boy's\",\n",
       " 'that',\n",
       " 'played',\n",
       " 'the',\n",
       " \"part's\",\n",
       " 'of',\n",
       " 'norman',\n",
       " 'and',\n",
       " 'paul',\n",
       " 'they',\n",
       " 'were',\n",
       " 'just',\n",
       " 'brilliant',\n",
       " 'children',\n",
       " 'are',\n",
       " 'often',\n",
       " 'left',\n",
       " 'out',\n",
       " 'of',\n",
       " 'the',\n",
       " 'praising',\n",
       " 'list',\n",
       " 'i',\n",
       " 'think',\n",
       " 'because',\n",
       " 'the',\n",
       " 'stars',\n",
       " 'that',\n",
       " 'play',\n",
       " 'them',\n",
       " 'all',\n",
       " 'grown',\n",
       " 'up',\n",
       " 'are',\n",
       " 'such',\n",
       " 'a',\n",
       " 'big',\n",
       " 'profile',\n",
       " 'for',\n",
       " 'the',\n",
       " 'whole',\n",
       " 'film',\n",
       " 'but',\n",
       " 'these',\n",
       " 'children',\n",
       " 'are',\n",
       " 'amazing',\n",
       " 'and',\n",
       " 'should',\n",
       " 'be',\n",
       " 'praised',\n",
       " 'for',\n",
       " 'what',\n",
       " 'they',\n",
       " 'have',\n",
       " 'done',\n",
       " \"don't\",\n",
       " 'you',\n",
       " 'think',\n",
       " 'the',\n",
       " 'whole',\n",
       " 'story',\n",
       " 'was',\n",
       " 'so',\n",
       " 'lovely',\n",
       " 'because',\n",
       " 'it',\n",
       " 'was',\n",
       " 'true',\n",
       " 'and',\n",
       " 'was',\n",
       " \"someone's\",\n",
       " 'life',\n",
       " 'after',\n",
       " 'all',\n",
       " 'that',\n",
       " 'was',\n",
       " 'shared',\n",
       " 'with',\n",
       " 'us',\n",
       " 'all']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_review(train_reviews[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Learn embeddings for reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "reviews = np.concatenate((train_reviews, test_reviews))\n",
    "docs = [TaggedDocument(decode_review(review), [i]) for i, review in enumerate(reviews)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class Doc2VecCallback(CallbackAny2Vec):\n",
    "    def __init__(self, epochs):\n",
    "        self.prog_bar = tf.keras.utils.Progbar(epochs)\n",
    "        self.epoch = 0\n",
    "    def on_epoch_end(self, model):\n",
    "        self.epoch += 1\n",
    "        self.prog_bar.update(self.epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 774s 8s/step\n"
     ]
    }
   ],
   "source": [
    "d2v_model = Doc2Vec(docs, dm=0, min_count=2, vector_size=100, hs=0, negative=5, epochs=100,\n",
    "                    callbacks=[Doc2VecCallback(100)], sample=0, workers=multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "embdgs = d2v_model.docvecs.vectors_docs\n",
    "train_embdgs, test_embdgs = np.split(embdgs, [25000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.08079641,  0.20569757,  0.4738193 ,  0.23749965,  0.06664906,\n",
       "        1.2267363 , -0.70511824,  0.48151103, -0.55024695, -0.14436685,\n",
       "       -0.23059061,  0.7129091 , -0.60188824,  0.5016063 ,  0.18376477,\n",
       "       -0.5230938 ,  0.16004896, -0.18659687,  0.8274295 ,  0.04011085,\n",
       "        0.03508369,  0.29871807,  0.12340536, -0.55743134,  0.06399595,\n",
       "       -0.5479066 , -0.89346504, -0.615669  , -0.05332805,  0.28452045,\n",
       "       -0.08361472, -0.82962734,  1.2487692 , -0.8348145 , -1.3827287 ,\n",
       "       -0.32844827, -0.05866596, -0.20214   ,  0.8929514 , -0.50951415,\n",
       "       -0.42142662,  0.2502974 , -0.5526857 , -0.01847663, -0.5334354 ,\n",
       "       -0.44521442,  0.00903169,  0.09517114, -0.06399161,  0.21078157,\n",
       "       -0.44145957,  0.79780304,  0.708781  ,  0.52510357,  0.6052623 ,\n",
       "        0.14815222, -0.5089591 ,  0.20163493, -1.6821849 , -0.6525678 ,\n",
       "       -0.20529775, -0.34921286, -0.91900027, -0.4330489 , -0.20630024,\n",
       "        0.02228682, -1.0429921 ,  0.07120833,  0.13347925, -0.16419138,\n",
       "       -0.16784236, -0.55934054, -0.56118524, -0.37115732,  0.04414184,\n",
       "        0.18220526,  0.4717986 ,  0.01929729, -0.10927698, -0.32006076,\n",
       "       -0.16162223,  0.6462481 , -0.6281219 , -1.134469  ,  0.10179093,\n",
       "        0.23171625,  0.01063073, -0.07949349,  0.27011207, -0.43695652,\n",
       "        0.16555595,  0.40691534,  0.34857702,  0.6036801 , -0.43055603,\n",
       "        0.393619  ,  0.11630932,  0.7341948 , -0.86189365, -0.06586093],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_embdgs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "embdgs_dir = \"embdgs\"\n",
    "os.mkdir(embdgs_dir)\n",
    "metadata_path = os.path.join(embdgs_dir, \"metadata.tsv\")\n",
    "embdgs_path = os.path.join(embdgs_dir, \"embdgs.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with open(metadata_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"review\")\n",
    "    for review in reviews:\n",
    "        excerpt = \" \".join(decode_review(review[1:31]))\n",
    "        f.write(f\"{excerpt}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Jack\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "tf.get_variable(\"embdgs\", initializer=embdgs)\n",
    "writer = tf.summary.FileWriter(embdgs_dir)\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "config = projector.ProjectorConfig()\n",
    "embdg_conf = config.embeddings.add()\n",
    "embdg_conf.tensor_name = \"embdgs\"\n",
    "embdg_conf.metadata_path = \"metadata.tsv\"\n",
    "projector.visualize_embeddings(writer, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "saver.save(sess, embdgs_path)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Classify reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(0.01), loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Jack\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/50\n",
      "25000/25000 [==============================] - 1s 24us/sample - loss: 0.3321 - acc: 0.8706\n",
      "Epoch 2/50\n",
      "25000/25000 [==============================] - 0s 15us/sample - loss: 0.2828 - acc: 0.8883\n",
      "Epoch 3/50\n",
      "25000/25000 [==============================] - 0s 15us/sample - loss: 0.2820 - acc: 0.8877\n",
      "Epoch 4/50\n",
      "25000/25000 [==============================] - 0s 16us/sample - loss: 0.2815 - acc: 0.8879\n",
      "Epoch 5/50\n",
      "25000/25000 [==============================] - 0s 16us/sample - loss: 0.2821 - acc: 0.8892\n",
      "Epoch 6/50\n",
      "25000/25000 [==============================] - 0s 15us/sample - loss: 0.2819 - acc: 0.8871\n",
      "Epoch 7/50\n",
      "25000/25000 [==============================] - 0s 18us/sample - loss: 0.2826 - acc: 0.8872\n",
      "Epoch 8/50\n",
      "25000/25000 [==============================] - 0s 18us/sample - loss: 0.2826 - acc: 0.8873\n",
      "Epoch 9/50\n",
      "25000/25000 [==============================] - 0s 16us/sample - loss: 0.2819 - acc: 0.8881\n",
      "Epoch 10/50\n",
      "25000/25000 [==============================] - 0s 16us/sample - loss: 0.2818 - acc: 0.8886\n",
      "Epoch 11/50\n",
      "25000/25000 [==============================] - 0s 18us/sample - loss: 0.2823 - acc: 0.8867\n",
      "Epoch 12/50\n",
      "25000/25000 [==============================] - 0s 16us/sample - loss: 0.2816 - acc: 0.8891\n",
      "Epoch 13/50\n",
      "25000/25000 [==============================] - 0s 16us/sample - loss: 0.2821 - acc: 0.8884\n",
      "Epoch 14/50\n",
      "25000/25000 [==============================] - 0s 15us/sample - loss: 0.2814 - acc: 0.8884\n",
      "Epoch 15/50\n",
      "25000/25000 [==============================] - 0s 17us/sample - loss: 0.2823 - acc: 0.8871\n",
      "Epoch 16/50\n",
      "25000/25000 [==============================] - 0s 18us/sample - loss: 0.2817 - acc: 0.8888\n",
      "Epoch 17/50\n",
      "25000/25000 [==============================] - 0s 15us/sample - loss: 0.2824 - acc: 0.8886\n",
      "Epoch 18/50\n",
      "25000/25000 [==============================] - 0s 16us/sample - loss: 0.2821 - acc: 0.8879\n",
      "Epoch 19/50\n",
      "25000/25000 [==============================] - 0s 17us/sample - loss: 0.2825 - acc: 0.8873\n",
      "Epoch 20/50\n",
      "25000/25000 [==============================] - 0s 16us/sample - loss: 0.2824 - acc: 0.8878\n",
      "Epoch 21/50\n",
      "25000/25000 [==============================] - 0s 15us/sample - loss: 0.2821 - acc: 0.8881\n",
      "Epoch 22/50\n",
      "25000/25000 [==============================] - 0s 16us/sample - loss: 0.2817 - acc: 0.8877\n",
      "Epoch 23/50\n",
      "25000/25000 [==============================] - 0s 19us/sample - loss: 0.2824 - acc: 0.8868\n",
      "Epoch 24/50\n",
      "25000/25000 [==============================] - 0s 15us/sample - loss: 0.2820 - acc: 0.8867\n",
      "Epoch 25/50\n",
      "25000/25000 [==============================] - 0s 17us/sample - loss: 0.2820 - acc: 0.8882\n",
      "Epoch 26/50\n",
      "25000/25000 [==============================] - 0s 16us/sample - loss: 0.2826 - acc: 0.8870\n",
      "Epoch 27/50\n",
      "25000/25000 [==============================] - 0s 15us/sample - loss: 0.2820 - acc: 0.8878\n",
      "Epoch 28/50\n",
      "25000/25000 [==============================] - 1s 21us/sample - loss: 0.2819 - acc: 0.8904\n",
      "Epoch 29/50\n",
      "25000/25000 [==============================] - 0s 16us/sample - loss: 0.2817 - acc: 0.8895\n",
      "Epoch 30/50\n",
      "25000/25000 [==============================] - 0s 16us/sample - loss: 0.2820 - acc: 0.8886\n",
      "Epoch 31/50\n",
      "25000/25000 [==============================] - 0s 18us/sample - loss: 0.2823 - acc: 0.8874\n",
      "Epoch 32/50\n",
      "25000/25000 [==============================] - 0s 16us/sample - loss: 0.2818 - acc: 0.8877\n",
      "Epoch 33/50\n",
      "25000/25000 [==============================] - 0s 16us/sample - loss: 0.2823 - acc: 0.8870\n",
      "Epoch 34/50\n",
      "25000/25000 [==============================] - 0s 16us/sample - loss: 0.2817 - acc: 0.8883\n",
      "Epoch 35/50\n",
      "25000/25000 [==============================] - 0s 20us/sample - loss: 0.2819 - acc: 0.8883\n",
      "Epoch 36/50\n",
      "25000/25000 [==============================] - 0s 18us/sample - loss: 0.2824 - acc: 0.8888\n",
      "Epoch 37/50\n",
      "25000/25000 [==============================] - 0s 15us/sample - loss: 0.2824 - acc: 0.8881\n",
      "Epoch 38/50\n",
      "25000/25000 [==============================] - 0s 16us/sample - loss: 0.2821 - acc: 0.88800s - loss: 0.2722 - acc: 0\n",
      "Epoch 39/50\n",
      "25000/25000 [==============================] - 0s 19us/sample - loss: 0.2819 - acc: 0.8879\n",
      "Epoch 40/50\n",
      "25000/25000 [==============================] - 0s 16us/sample - loss: 0.2818 - acc: 0.8874\n",
      "Epoch 41/50\n",
      "25000/25000 [==============================] - 0s 17us/sample - loss: 0.2822 - acc: 0.8886\n",
      "Epoch 42/50\n",
      "25000/25000 [==============================] - 0s 16us/sample - loss: 0.2819 - acc: 0.8885\n",
      "Epoch 43/50\n",
      "25000/25000 [==============================] - 0s 16us/sample - loss: 0.2821 - acc: 0.8882\n",
      "Epoch 44/50\n",
      "25000/25000 [==============================] - 0s 16us/sample - loss: 0.2821 - acc: 0.8886\n",
      "Epoch 45/50\n",
      "25000/25000 [==============================] - 0s 15us/sample - loss: 0.2822 - acc: 0.8881\n",
      "Epoch 46/50\n",
      "25000/25000 [==============================] - 0s 16us/sample - loss: 0.2823 - acc: 0.8889\n",
      "Epoch 47/50\n",
      "25000/25000 [==============================] - 0s 16us/sample - loss: 0.2819 - acc: 0.8883\n",
      "Epoch 48/50\n",
      "25000/25000 [==============================] - 0s 16us/sample - loss: 0.2819 - acc: 0.8886\n",
      "Epoch 49/50\n",
      "25000/25000 [==============================] - 0s 15us/sample - loss: 0.2821 - acc: 0.8872\n",
      "Epoch 50/50\n",
      "25000/25000 [==============================] - 0s 16us/sample - loss: 0.2821 - acc: 0.8880\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x212aa3d15c0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_embdgs, train_labels, batch_size=64, epochs=50, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 1s 20us/sample - loss: 0.2794 - acc: 0.8875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2794467719936371, 0.88748]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_embdgs, test_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
